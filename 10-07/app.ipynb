{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e563541-d1b4-4f14-b88d-235464ef4c9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.5)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
      "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.41.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.3)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.10/dist-packages (2.6.2.2)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
      "Requirement already satisfied: loralib in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
      "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.0)\n",
      "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (15.0.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.24.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (4.24.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch datasets transformers peft trl bitsandbytes scipy tensorboardX accelerate loralib sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8555343-1ca6-410d-94df-661c3b9a8282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "517c1266-0ff1-4b1f-885d-1176aea3c698",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lmsys/longchat-13b-16k\"\n",
    "# The instruction dataset to use\n",
    "# dataset_name = \"dataset/data1\"\n",
    "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "dataset_name = \"./dataset\"\n",
    "# Fine-tuned model name\n",
    "new_model = \"llama-2-13b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "65fd3a78-f4ae-461b-a183-95f8501a4718",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_r = 64\n",
    "\n",
    "# Alpha parameter for LoRA scaling\n",
    "lora_alpha = 16\n",
    "\n",
    "# Dropout probability for LoRA layers\n",
    "lora_dropout = 0.1\n",
    "\n",
    "################################################################################\n",
    "# bitsandbytes parameters\n",
    "################################################################################\n",
    "\n",
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "################################################################################\n",
    "# TrainingArguments parameters\n",
    "################################################################################\n",
    "\n",
    "# Output directory where the model predictions and checkpoints will be stored\n",
    "output_dir = \"./results\"\n",
    "\n",
    "# Number of training epochs\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Enable fp16/bf16 training (set bf16 to True with an A100)\n",
    "fp16 = False\n",
    "bf16 = True\n",
    "\n",
    "# Batch size per GPU for training\n",
    "per_device_train_batch_size = 2\n",
    "\n",
    "# Batch size per GPU for evaluation\n",
    "per_device_eval_batch_size = 2\n",
    "\n",
    "# Number of update steps to accumulate the gradients for\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "gradient_checkpointing = True\n",
    "\n",
    "# Maximum gradient normal (gradient clipping)\n",
    "max_grad_norm = 0.3\n",
    "\n",
    "# Initial learning rate (AdamW optimizer)\n",
    "learning_rate = 2e-4\n",
    "\n",
    "# Weight decay to apply to all layers except bias/LayerNorm weights\n",
    "weight_decay = 0.001\n",
    "\n",
    "# Optimizer to use\n",
    "optim = \"paged_adamw_32bit\"\n",
    "\n",
    "# Learning rate schedule (constant a bit better than cosine)\n",
    "lr_scheduler_type = \"constant\"\n",
    "\n",
    "# Number of training steps (overrides num_train_epochs)\n",
    "max_steps = -1\n",
    "\n",
    "# Ratio of steps for a linear warmup (from 0 to learning rate)\n",
    "warmup_ratio = 0.03\n",
    "\n",
    "# Group sequences into batches with same length\n",
    "# Saves memory and speeds up training considerably\n",
    "group_by_length = True\n",
    "\n",
    "# Save checkpoint every X updates steps\n",
    "save_steps = 500\n",
    "\n",
    "# Log every X updates steps\n",
    "logging_steps = 500\n",
    "\n",
    "################################################################################\n",
    "# SFT parameters\n",
    "################################################################################\n",
    "\n",
    "# Maximum sequence length to use\n",
    "max_seq_length = None\n",
    "\n",
    "# Pack multiple short examples in the same input sequence to increase efficiency\n",
    "packing = False\n",
    "\n",
    "# Load the entire model on the GPU 0\n",
    "device_map = {\"\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1427fd4c-b843-43d2-8a79-135cfdbc70cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(dataset_name, split=\"train\")\n",
    "dataset = load_dataset(\"json\", data_files=\"./data/5.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e11eb3-8629-4327-9c7b-03e9dd448547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Your GPU supports bfloat16: accelerate training with bf16=True\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=use_4bit,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    ")\n",
    "\n",
    "# Check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and use_4bit:\n",
    "    major, _ = torch.cuda.get_device_capability()\n",
    "    if major >= 8:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
    "        print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3d716a-b79b-4f66-8d5e-ada1aff7c11a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dbb086e5ed4cc48908ae13acb9bc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "\n",
    "# Load LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf12fc9f-bf67-444e-9320-2ae1e3f41e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:122: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# Set supervised fine-tuning parameters\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=4049,\n",
    "    # max_seq_length=509,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bb4c1cb-832a-4c19-80c5-2baad20081d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='714' max='714' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [714/714 1:36:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.615700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=714, training_loss=1.5156947544642858, metrics={'train_runtime': 5776.4428, 'train_samples_per_second': 0.247, 'train_steps_per_second': 0.124, 'total_flos': 3.319205230540493e+17, 'train_loss': 1.5156947544642858, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb50955d-02bb-43d2-ace7-d51d1f2dd87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47de263f-58ac-4c48-96ad-4c07ff30ee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"traning is finished1111111111111111111111111111111\")\n",
    "\n",
    "print(\"----------------Test------------------\")\n",
    "prompt = \"TASK INSTRUCTIONS:\\n\\nGiven a list of clusters of texts describing people's experiences, classify which clusters show a shared personal experience across the different texts, and those that do not. If a cluster shows a shared experience, give it the tag 'shared'. if a cluster does not show a shared experience, and rather shows a mix of experiences, give it the tag 'mix'. Finally, if a cluster seems to have been repeated, give it a tag 'x similar to y' and provide the cluster numbers for x and y respectively.\\n\\nCLUSTERS TO CLASSIFY:\\n\\n0\\nUser asks for a password reset.\\nUser asks to reset two-factor authentication.\\nCustomer asks for more information regarding what issue exactly is when trying to log in.\\nWants a password reset.\\nCan you please help me to reset password?\\nRequests reset of password.\\nWants help logging into her account.\\nWants help with password.\\n1\\nLocked account was unlocked.\\nAccount got locked due to unsuccessful login attempts, so they unlocked it.\\nAccount unlocked due to unsuccessful login attempts.\\nAccount got locked due to unsuccessful login attempts.\\n2\\nTo complete the request, they require a description outlining the purpose of the account.\\nUser requests a description outlining the purpose of the new account.\\nTo complete request, customer needs to describe the purpose of a new checking account.\\n3\\nCustomer asks Rho bank team to reset password.\\nWould like Rho account help to recover credentials.\\nUser wants to update their address with Rho.\\n4\\nCustomer service representative asks customer to try to log in again, to confirm personal details.\\nUser asks Boris to check phone for an SMS with a code, to try it to log in.\\nCustomer service representative unlocks customer's account after it had been locked due to entering the wrong password.\\n5\\nUser asks to add a checking account to Bonside Advisor LLC.\\nUser wants to add a checking account to their service.\\n6\\nUser asks for a reset password link one more time, to save to all devices.\\nUser wants reset password link sent again.\\n7\\nUser requests help clearing browsing history, cache and cookies.\\nUser asks customer to clear browsing history, cache and cookies, then close and reopen browser.\\n\\n###\\n\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
    "result = pipe(f\"{prompt}\")\n",
    "\n",
    "print(\"result is:111111111\")\n",
    "print(result[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9dbfa4-b3ca-4fe1-bc32-5829bebbd774",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------Test------------------\")\n",
    "prompt = \"TASK INSTRUCTIONS:\\n\\nGiven a list of clusters of texts describing people's experiences, classify which clusters show a shared personal experience across the different texts, and those that do not. If a cluster shows a shared experience, give it the tag 'shared'. if a cluster does not show a shared experience, and rather shows a mix of experiences, give it the tag 'mix'. Finally, if a cluster seems to have been repeated, give it a tag 'x similar to y' and provide the cluster numbers for x and y respectively.\\n\\nCLUSTERS TO CLASSIFY:\\n\\n0\\nUser asks for a password reset.\\nUser asks to reset two-factor authentication.\\nCustomer asks for more information regarding what issue exactly is when trying to log in.\\nWants a password reset.\\nCan you please help me to reset password?\\nRequests reset of password.\\nWants help logging into her account.\\nWants help with password.\\n1\\nLocked account was unlocked.\\nAccount got locked due to unsuccessful login attempts, so they unlocked it.\\nAccount unlocked due to unsuccessful login attempts.\\nAccount got locked due to unsuccessful login attempts.\\n2\\nTo complete the request, they require a description outlining the purpose of the account.\\nUser requests a description outlining the purpose of the new account.\\nTo complete request, customer needs to describe the purpose of a new checking account.\\n3\\nCustomer asks Rho bank team to reset password.\\nWould like Rho account help to recover credentials.\\nUser wants to update their address with Rho.\\n4\\nCustomer service representative asks customer to try to log in again, to confirm personal details.\\nUser asks Boris to check phone for an SMS with a code, to try it to log in.\\nCustomer service representative unlocks customer's account after it had been locked due to entering the wrong password.\\n5\\nUser asks to add a checking account to Bonside Advisor LLC.\\nUser wants to add a checking account to their service.\\n6\\nUser asks for a reset password link one more time, to save to all devices.\\nUser wants reset password link sent again.\\n7\\nUser requests help clearing browsing history, cache and cookies.\\nUser asks customer to clear browsing history, cache and cookies, then close and reopen browser.\\n\\n###\\n\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=4000)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(\"result is:111111111\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aad01d-f24f-44cd-9a2a-240398155cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c4366-bd43-491f-abe3-09d2a052d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------Test------------------\")\n",
    "prompt = \"TASK INSTRUCTIONS:\\n\\nGiven a list of clusters of texts describing people's experiences, classify which clusters show a shared personal experience across the different texts, and those that do not. If a cluster shows a shared experience, give it the tag 'shared'. if a cluster does not show a shared experience, and rather shows a mix of experiences, give it the tag 'mix'. Finally, if a cluster seems to have been repeated, give it a tag 'x similar to y' and provide the cluster numbers for x and y respectively.\\n\\nCLUSTERS TO CLASSIFY:\\n\\n0\\nIs Batman experience suitable for a child?\\nIs Batman experience available, or only the regular one?\\nAre these events for kids 6-12?\\nWhat time does the Batman experience stop?\\nWhy is the Batman experience not recommended for children?\\nCan user be admitted to Batman experience at 7:05?\\n1\\nIs there a way to still obtain a refund, or a voucher to complete visit on a later date?\\nHow to get refund for cancelled flight?\\nIs a ticket refundable if purchased?\\nWhat is the refund policy?\\nCan I get a refund for three tickets bought for today?\\n2\\nCan the gift pass be used to visit the SpyScape exhibit?\\nIs Spyscape open every day during a trip to NYC from Dec 24-28?\\nWill Spyscape experience be interesting for someone with basic English?\\nWhat's the difference between the All Access Pass and the Spy Date Experience Pass?\\nIs Spyscape suitable for age 6?\\n3\\nCan customer come in by 1:30, or will have to reschedule?\\nIf he purchases the tickets with a specific date, can his friends reschedule for another date in December?\\nIs it too late to change a group's time slot, or will they be denied entrance?\\nCan user change the date of tickets she bought?\\n4\\nHow to use 2 for 1 ticket offer through NYCgo?\\nCan school buy tickets upon arrival?\\nHow to buy the discounted tickets?\\nCan I buy my child's ticket online in a separate order?\\n5\\nDo you have a place to store their backpacks while they take the tour?\\nDoes the venue offer bag checks?\\nDoes company offer a coat check or locker to put items\\/coats away?\\n6\\nCan they make a reservation with a \\\"Sightseeing Pass\\\"?\\nHow to make reservations for a visit from Jan 1st to 3rd, with Go City Explorer Pass?\\nDoes user need to make reservations in advance for the Sightseeing Pass, or can they just show up and scan when they get there?\\n7\\nCan they buy tickets when they arrive at the museum, or should they buy them in advance?\\nDo I need to make a reservation in advance to visit the museum?\\nDo I have to buy tickets in advance if I want to visit with my family on December 24?\\n8\\nIs there a problem purchasing tickets on the website?\\nCan date of gift tickets be extended?\\nIf it is closed, why would it allow purchase of tickets for a closed day?\\n9\\nCustomer asks how to get the tickets to ensure they have the same timeslot as the larger group.\\nIs there an age minimum for the VIP?\\nWhat ticket type did customer want to purchase?\\nIf group purchased VIP tickets together, can they all go in together, or will they be split up?\\n10\\nIs it OK if a reservation is late, due to traffic from New Jersey?\\nIs it okay if a customer arrives a few minutes late for a scheduled meeting?\\nIs being late for a scheduled appointment a problem?\\nHow late arrival to reservation affects reservation?\\n11\\nCan they find out if there are open spots without having to show up in person?\\nCan they pre-book a time\\/date, or need to wait in line?\\n\\n###\\n\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=509)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "\n",
    "print(\"result is:111111111\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "print(\"----------------Test------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71fe116-9ed2-454d-ac62-27c7471c7e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----------------Test------------------\")\n",
    "prompt = \"TASK INSTRUCTIONS:\\n\\nGiven a list of clusters of texts describing people's experiences, classify which clusters show a shared personal experience across the different texts, and those that do not. If a cluster shows a shared experience, give it the tag 'shared'. if a cluster does not show a shared experience, and rather shows a mix of experiences, give it the tag 'mix'. Finally, if a cluster seems to have been repeated, give it a tag 'x similar to y' and provide the cluster numbers for x and y respectively.\\n\\nCLUSTERS TO CLASSIFY:\\n\\n0\\nThe more quickly something can be done, the more quickly the user will learn to do more.\\nCommunication\\/coordination worked well for family's needs.\\nUnlocking the use of the site for French-speaking people helps them use it optimally.\\nPwC's consulting arm, ProEdge, sells ProEdge tooling and services.\\n1\\nJefit includes nutrition tracking, goals, coaching, etc.\\nJefit is the top exercise tracker.\\nWill help user put more accurate win\\/loss in sport events.\\nAllows better \\\"at a glance\\\" analytics.\\n2\\nThe product provides the ability to run a series of actions all at once, and efficiently print certain pages of a document without needing to point to everyone where the print button is.\\nEnables user to only expose information their external partner needs to know.\\nFeature is valuable because it automatically creates a link back to the source.\\nGoogle Docs feature of seeing which changes were made is valuable for team collaboration, allowing decisions to be more thought-out.\\n3\\nWebCatalog appifies many of these sorts of apps, and it works really well.\\nComplete agreement with previous feedback about the need for more powerful iOS app, full capabilities on the iPad, and more nativeness or independence per platform.\\nThe product allows a more seamless user experience when using external websites for functionality.\\nRuns webpage in standalone Chrome instance.\\n4\\nSoftware adds a lot of value to user.\\nThinks design is sleek and like notion.\\nFeature allows startup to personalize workspace, reinforcing brand, becoming an expectation for tools alike.\\nSo far, product is great, user loves it, and would like to be part of the team one day.\\n5\\nUser finds the product easy to use.\\nSays product is great.\\nThe rest of the product is perfect.\\nSays product is great.\\n6\\nOnce user understood that all building blocks were nested in the Explore button, it was easier to use.\\nNew features make project trackers more useful, fresh.\\nMove line and Insert line are very welcome additions to the context menu.\\nMakes more matching dashboards for clients.\\n7\\nDecides to move personal and professional websites to Coda.\\nAbility to move graphical objects around on the canvas to set values in coda tables.\\nSurprised to find Coda can be tailored to replace or complement existing systems.\\nCODA is the solution for small businesses.\\n8\\nUser loves Coda so far.\\nPrefers Notion's templates over Coda's.\\nStill use Coda.\\nGreat to hear that Max's customer loves the flexibility of most other aspects of Coda.\\n9\\nCoda is so versatile, user is impressed with the way the other person found a way to do anything in it.\\nCoda allows managing large datasets spread across multiple views of the same presentation more manageable.\\nCoda is intuitive to learn, and helpful videos are useful.\\nCoda is much better than anything else, and customer support is outstanding.\\n10\\nLikes Notion for personal use.\\nLikes Notion's page structure.\\nNotion's capabilities are also profoundly useful and gratifying.\\nWas helpful to keep all content in one place with Super.so and Notion as a data backend.\\n11\\nInteresting idea!\\nUser likes the tool, but would like more freedom, such as the ability to remove emojis from the title if desired, and more font sizes and colors.\\nUser finds it cool that they can make their own graphs.\\nFound it very interesting to read about the work-around you built using Kumu, and enjoyed both of your thoughts on the topic.\\n12\\nUser thinks the join function does exactly what they expect, and that's because their expectation is shaped by other \\\"joiny\\\" functions they're familiar with.\\nTaking ambiguity out of data visualization.\\nThe attachment column is better in Airtable than in Basecamp.\\nThinks Detail view is a nice way to show data.\\n13\\nMethod works as long as rows are never sorted or manually changed to cause RowID values to be out of order.\\nThe formula works for the user quite well.\\nThe product adds value to understanding and maintaining formulas.\\nThe formulas are easy to learn.\\n\\n###\\n\\n\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=4000)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "\n",
    "print(\"result is:111111111\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "\n",
    "print(\"----------------Test------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a2dde-c337-4e56-8f4e-85ee8bdb55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dbf10f-c4b4-4ebc-a16a-57c1b6ddd56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    " model_name,\n",
    " low_cpu_mem_usage=True,\n",
    " return_dict=True,\n",
    " torch_dtype=torch.float16,\n",
    " device_map=device_map,\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    " \n",
    "model.save_pretrained(\"merged_model\",safe_serialization=True)\n",
    "tokenizer.save_pretrained(\"merged_model\")\n",
    " \n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "!huggingface-cli login --token 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\n",
    "new_model = 'space505/llama2-23b-16k'\n",
    "model.push_to_hub(new_model, use_temp_dir=False, create_pr=1)\n",
    "tokenizer.push_to_hub(new_model, use_temp_dir=False, create_pr=1)\n",
    "\n",
    "print('huggingface pushed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9bce6-b72a-41cf-8b1e-d036c7c07a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce82a4-d5da-4eaf-b9a5-1df71055a916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d516a517-9ce6-4826-a570-9b09be2836e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da74407-0107-4d75-a10a-05152a21f6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7425e243-dbdf-41c9-b5cf-cbcf044136f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
